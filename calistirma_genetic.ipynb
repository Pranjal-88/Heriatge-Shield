{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e5ca944",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# PAKETLER\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "from sklearn.metrics import max_error, explained_variance_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM, GRU, SimpleRNN, TimeDistributed, Bidirectional, ConvLSTM2D, Dropout\n",
    "from tensorflow.keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "799c2cae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-28T22:53:14.743117Z",
     "iopub.status.busy": "2022-01-28T22:53:14.740119Z",
     "iopub.status.idle": "2022-01-28T22:53:14.754004Z",
     "shell.execute_reply": "2022-01-28T22:53:14.754004Z"
    },
    "executionInfo": {
     "elapsed": 692,
     "status": "ok",
     "timestamp": 1643404023507,
     "user": {
      "displayName": "Caner ERDEN",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhzPeinEBGlPvuQy-mLSs1nVsVu8vma6Q8f1ftyaQ=s64",
      "userId": "10580483392958313513"
     },
     "user_tz": -180
    },
    "id": "ixKsBn9aMe2O",
    "papermill": {
     "duration": 0.032855,
     "end_time": "2022-01-28T22:53:14.755003",
     "exception": false,
     "start_time": "2022-01-28T22:53:14.722148",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# değişecek parametreler\n",
    "model_number = str()\n",
    "learning_rate = 0.01\n",
    "dropout_rate = 0\n",
    "layer_size = 1\n",
    "units = 1\n",
    "activation_function = 'relu'\n",
    "loss_function = 'mae'\n",
    "epoch = 1\n",
    "batch_size = 1\n",
    "models = ['LSTM','GRU']\n",
    "optimizer = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce7a5481",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if optimizers == 'SGD':\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, clipnorm=1)\n",
    "elif optimizers == 'Adam':\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, clipnorm=1)\n",
    "elif optimizers == 'Adamax':\n",
    "    optimizer = tf.keras.optimizers.Adamax(learning_rate=learning_rate, clipnorm=1)\n",
    "elif optimizers == 'Adagrad':\n",
    "    optimizer = tf.keras.optimizers.Adagrad(learning_rate=learning_rate, clipnorm=1)\n",
    "elif optimizers == 'RMSprop':\n",
    "    optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate, clipnorm=1)\n",
    "elif optimizers == 'Adadelta':\n",
    "    optimizer = tf.keras.optimizers.Adadelta(learning_rate=learning_rate, clipnorm=1)\n",
    "elif optimizers == 'Nadam':\n",
    "    optimizer = tf.keras.optimizers.Nadam(learning_rate=learning_rate, clipnorm=1)\n",
    "elif optimizers == 'Ftrl':\n",
    "    optimizer = tf.keras.optimizers.Ftrl(learning_rate=learning_rate, clipnorm=1)\n",
    "# Değiştirilmeyen parametreler\n",
    "return_sequence = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "722a6db6",
   "metadata": {
    "code_folding": [
     483
    ],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEED Ayarlandı:  12345\n",
      "(43033, 240) x (43033,) y\n",
      "(30123, 24, 10) X_train.shape\n",
      "(12910, 24, 10) X_test.shape\n",
      "(30123,) y_train_df.shape\n",
      "(12910,) y_test_df.shape\n",
      "Training.......\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown optimizer: ''. Please ensure you are using a `keras.utils.custom_object_scope` and that this object is included in the scope. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 450\u001b[0m\n\u001b[0;32m    448\u001b[0m start_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m    449\u001b[0m \u001b[38;5;66;03m# Öğrenme Aşaması\u001b[39;00m\n\u001b[1;32m--> 450\u001b[0m history1, model1 \u001b[38;5;241m=\u001b[39m \u001b[43mfit_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    451\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    453\u001b[0m end_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m    454\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDuration: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(end_time \u001b[38;5;241m-\u001b[39m start_time))\n",
      "Cell \u001b[1;32mIn[10], line 308\u001b[0m, in \u001b[0;36mfit_model\u001b[1;34m(layer_size, X_train, X_test, y_train_df, batch_size, epoch, units, activation_function, loss_function, optimizer, layer)\u001b[0m\n\u001b[0;32m    306\u001b[0m         model\u001b[38;5;241m.\u001b[39madd(Dropout(dropout_rate))\n\u001b[0;32m    307\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dense(\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m--> 308\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    309\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(X_train,\n\u001b[0;32m    310\u001b[0m                     y_train_df\u001b[38;5;241m.\u001b[39mto_numpy(),\n\u001b[0;32m    311\u001b[0m                     epochs\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    314\u001b[0m                     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    315\u001b[0m                     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m history, model\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\saving\\legacy\\serialization.py:368\u001b[0m, in \u001b[0;36mclass_and_config_for_serialized_keras_object\u001b[1;34m(config, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[0;32m    364\u001b[0m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m object_registration\u001b[38;5;241m.\u001b[39mget_registered_object(\n\u001b[0;32m    365\u001b[0m     class_name, custom_objects, module_objects\n\u001b[0;32m    366\u001b[0m )\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 368\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    369\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprintable_module_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    370\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure you are using a `keras.utils.custom_object_scope` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    371\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand that this object is included in the scope. See \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    372\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.tensorflow.org/guide/keras/save_and_serialize\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    373\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#registering_the_custom_object for details.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    374\u001b[0m     )\n\u001b[0;32m    376\u001b[0m cls_config \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    377\u001b[0m \u001b[38;5;66;03m# Check if `cls_config` is a list. If it is a list, return the class and the\u001b[39;00m\n\u001b[0;32m    378\u001b[0m \u001b[38;5;66;03m# associated class configs for recursively deserialization. This case will\u001b[39;00m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;66;03m# happen on the old version of sequential model (e.g. `keras_version` ==\u001b[39;00m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;66;03m# \"2.0.6\"), which is serialized in a different structure, for example\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;66;03m# \"{'class_name': 'Sequential',\u001b[39;00m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;66;03m#   'config': [{'class_name': 'Embedding', 'config': ...}, {}, ...]}\".\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Unknown optimizer: ''. Please ensure you are using a `keras.utils.custom_object_scope` and that this object is included in the scope. See https://www.tensorflow.org/guide/keras/save_and_serialize#registering_the_custom_object for details."
     ]
    }
   ],
   "source": [
    "SEED = 12345\n",
    "# Diğer Değişkenler\n",
    "target = 'PM2_5'\n",
    "n_hours = 24\n",
    "train_size = 0.7\n",
    "\n",
    "# Aynı sonuçlar üretebilmek için\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "print(\"SEED Ayarlandı: \", SEED)\n",
    "\n",
    "cols = [\n",
    "    \"PM2_5\",\n",
    "    \"SO2\",\n",
    "    \"NO\",\n",
    "    \"NO2\",\n",
    "    \"NOX\",\n",
    "    \"O3\",\n",
    "    \"wind_speed\",\n",
    "    \"relative_humidity\",\n",
    "    \"air_pressure\",\n",
    "]\n",
    "cols_to_analyze = [\n",
    "    \"PM2_5\",\n",
    "    \"SO2\",\n",
    "    \"NO\",\n",
    "    \"NO2\",\n",
    "    \"NOX\",\n",
    "    \"O3\",\n",
    "    \"temperature\",\n",
    "    \"wind_speed\",\n",
    "    \"relative_humidity\",\n",
    "    \"air_pressure\",\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# FONKSİYONLAR\n",
    "def get_data(data_name_xlsx):\n",
    "    raw_data = pd.read_excel(data_name_xlsx)\n",
    "    raw_data['date'] = pd.to_datetime(raw_data['date'],\n",
    "                                      format='%d.%m.%Y %H:%M')\n",
    "    raw_data.index = raw_data['date']\n",
    "    raw_data.drop('date', axis='columns', inplace=True)\n",
    "    raw_data_2015 = raw_data.loc[raw_data.index >= pd.to_datetime(\n",
    "        '01.01.2015 00:00', format='%d.%m.%Y %H:%M')]\n",
    "    return raw_data_2015\n",
    "\n",
    "\n",
    "def remove_outliers(dataset, cols):\n",
    "    # sıfırdan küçük olan aşağıdaki sütun değerlerini 0 yap.\n",
    "    # Sıfır değerlerini eksik veri olarak kaydet.\n",
    "    dataset[cols] = dataset[cols].clip(lower=0)\n",
    "    dataset[cols] = dataset[cols].replace(0, np.nan)\n",
    "    # rüzgar değişkenini kategorik veri yap KD: KuzeyDoğu\n",
    "    #wind_categories = pd.cut(\n",
    "    #    data.wind, bins=[-1, 90, 180, 270, 361], labels=[\"KD\", \"KB\", \"GB\", \"GD\"])\n",
    "    # data[\"wind_category\"] = wind_categories\n",
    "    # data.pop(\"wind\")\n",
    "    # dataset = pd.get_dummies(data, columns=[\"wind_category\"])\n",
    "\n",
    "    # Aykırı verileri de eksik veri olarak kaydet\n",
    "    for col in cols:\n",
    "        Q1 = dataset[col].quantile(0.25)\n",
    "        Q3 = dataset[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        # print(\"col: {}, Q1: {}, Q3: {}, IQR: {}\".format(col, Q1, Q3, IQR))\n",
    "        ust_limit = Q3 + 3 * IQR\n",
    "        alt_limit = Q1 - 3 * IQR\n",
    "        # print(\"ust_limit: {}, alt_limit: {}\".format(ust_limit, alt_limit))\n",
    "        dataset.loc[dataset[col] >= ust_limit, col] = np.nan\n",
    "        dataset.loc[dataset[col] <= alt_limit, col] = np.nan\n",
    "    dataset = dataset.replace(-9999, np.nan)\n",
    "    dataset['temperature'] = dataset['temperature'].replace(0, np.nan)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def impute_data(data):\n",
    "    filled_data = data.interpolate(method='linear', axis=0).ffill().bfill()\n",
    "    return filled_data\n",
    "\n",
    "\n",
    "# convert series to supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j + 1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j + 1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j + 1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n",
    "\n",
    "\n",
    "def scale_data(dataset):\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled_df = pd.DataFrame(scaler.fit_transform(dataset),\n",
    "                             index=dataset.index,\n",
    "                             columns=dataset.columns)\n",
    "    return scaler, scaled_df\n",
    "\n",
    "\n",
    "def create_dataset(data_name_xlsx, cols, n_hours, target, train_size):\n",
    "    \"\"\"\n",
    "    data_name_xlsx: Excel dosyasının adı\n",
    "    cols: özellikler listesi\n",
    "    n_hours: kaç saatlik veri girdi olarak verilecek\n",
    "    target: hangi özellik tahmin edilecek\n",
    "    train_size: eğitim oranı\n",
    "    \"\"\"\n",
    "    # load data\n",
    "    raw_data = get_data(data_name_xlsx)\n",
    "    # özellikler seti\n",
    "    raw_data = raw_data[cols]\n",
    "    # remove outliers\n",
    "    raw_data_missing = remove_outliers(raw_data, cols)\n",
    "    # raw_data_missing.to_csv(\"raw_data_missing.csv\")\n",
    "    # Eksik verilerin doldurulması\n",
    "    filled_data = impute_data(raw_data_missing)\n",
    "    filled_data.to_csv(\"data/filled_data.csv\")\n",
    "    # scaling\n",
    "    scaler, scaled_df = scale_data(filled_data)\n",
    "    # Veri çerçeveleme\n",
    "    reframed_df = pd.DataFrame(\n",
    "        series_to_supervised(scaled_df.to_numpy(), n_hours, len([target])))\n",
    "    # Düşürülecek sütunlar sondan özellik sayısından 1 eksik olana kadar gideceğiz.\n",
    "    reframed_df = reframed_df.iloc[:, :-(len(cols) - 1)]\n",
    "    reframed_df.index = filled_data.index[n_hours:]\n",
    "    # reframed_df.to_csv(\"reframed.csv\")\n",
    "    # split_data\n",
    "    X = reframed_df.iloc[:, :-1]\n",
    "    y = reframed_df.iloc[:, -1]\n",
    "    print(X.shape, \"x\", y.shape, \"y\")\n",
    "    X_train_df, X_test_df, y_train_df, y_test_df = train_test_split(\n",
    "        X, y, train_size=train_size, shuffle=False)\n",
    "    X_train = X_train_df.to_numpy().reshape(\n",
    "        (X_train_df.shape[0], n_hours, scaled_df.shape[1]))\n",
    "    X_test = X_test_df.to_numpy().reshape(\n",
    "        (X_test_df.shape[0], n_hours, scaled_df.shape[1]))\n",
    "    print(X_train.shape, \"X_train.shape\")\n",
    "    print(X_test.shape, \"X_test.shape\")\n",
    "    print(y_train_df.shape, \"y_train_df.shape\")\n",
    "    print(y_test_df.shape, \"y_test_df.shape\")\n",
    "    return X_train, X_test, y_train_df, y_test_df\n",
    "\n",
    "\n",
    "def fit_model(layer_size, X_train, X_test, y_train_df, batch_size, epoch,\n",
    "              units, activation_function, loss_function, optimizer, layer):\n",
    "    if layer_size == 1:  # 1 gizli katman varsa\n",
    "        model = Sequential()\n",
    "        if layer[0] == 'GRU':\n",
    "            model.add(\n",
    "                GRU(units,\n",
    "                    activation=activation_function,\n",
    "                    input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "        elif layer[0] == 'LSTM':\n",
    "            model.add(\n",
    "                LSTM(units,\n",
    "                     activation=activation_function,\n",
    "                     input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "        elif layer[0] == 'SimpleRNN':\n",
    "            model.add(\n",
    "                SimpleRNN(units,\n",
    "                          activation=activation_function,\n",
    "                          input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "\n",
    "    elif layer_size == 2:  # 2 gizli katman varsa\n",
    "        model = Sequential()\n",
    "        first_layer = layer[0][0]\n",
    "        second_layer = layer[0][1]\n",
    "        if (first_layer == 'GRU') and (second_layer == 'GRU'):\n",
    "            print(\"GRU GRU\")\n",
    "            model.add(\n",
    "                GRU(units,\n",
    "                    activation=activation_function,\n",
    "                    input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                    return_sequences=True))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "            model.add(GRU(units, activation=activation_function))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "        elif (first_layer == 'LSTM') and (second_layer == 'LSTM'):\n",
    "            model.add(\n",
    "                LSTM(units,\n",
    "                     activation=activation_function,\n",
    "                     input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                     return_sequences=True))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "            model.add(LSTM(units, activation=activation_function))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "        elif (first_layer == 'SimpleRNN') and (second_layer == 'SimpleRNN'):\n",
    "            model.add(\n",
    "                SimpleRNN(units,\n",
    "                          activation=activation_function,\n",
    "                          input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                          return_sequences=True))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "            model.add(SimpleRNN(units, activation=activation_function))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "        elif (first_layer == 'GRU') and (second_layer == 'LSTM'):\n",
    "            model.add(\n",
    "                GRU(units,\n",
    "                    activation=activation_function,\n",
    "                    input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                    return_sequences=True))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "            model.add(LSTM(units, activation=activation_function))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "        elif (first_layer == 'GRU') and (second_layer == 'SimpleRNN'):\n",
    "            model.add(\n",
    "                GRU(units,\n",
    "                    activation=activation_function,\n",
    "                    input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                    return_sequences=True))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "            model.add(SimpleRNN(units, activation=activation_function))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "        elif (first_layer == 'LSTM') and (second_layer == 'SimpleRNN'):\n",
    "            model.add(\n",
    "                LSTM(units,\n",
    "                     activation=activation_function,\n",
    "                     input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                     return_sequences=True))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "            model.add(SimpleRNN(units, activation=activation_function))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "        elif (first_layer == 'LSTM') and (second_layer == 'GRU'):\n",
    "            model.add(\n",
    "                LSTM(units,\n",
    "                     activation=activation_function,\n",
    "                     input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                     return_sequences=True))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "            model.add(GRU(units, activation=activation_function))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "        elif (first_layer == 'SimpleRNN') and (second_layer == 'GRU'):\n",
    "            model.add(\n",
    "                SimpleRNN(units,\n",
    "                          activation=activation_function,\n",
    "                          input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                          return_sequences=True))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "            model.add(GRU(units, activation=activation_function))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "        elif (first_layer == 'SimpleRNN') and (second_layer == 'LSTM'):\n",
    "            model.add(\n",
    "                SimpleRNN(units,\n",
    "                          activation=activation_function,\n",
    "                          input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                          return_sequences=True))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "            model.add(LSTM(units, activation=activation_function))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "\n",
    "    else:  # 4 gizli katman varsa\n",
    "        model = Sequential()\n",
    "        if layer[0] == 'GRU':\n",
    "            model.add(\n",
    "                GRU(units,\n",
    "                    activation=activation_function,\n",
    "                    input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                    return_sequences=True))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "            model.add(GRU(units, activation=activation_function, return_sequences=True))\n",
    "            model.add(GRU(units, activation=activation_function, return_sequences=True))\n",
    "            model.add(GRU(units, activation=activation_function))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "        elif layer[0] == 'LSTM':\n",
    "            model.add(\n",
    "                LSTM(units,\n",
    "                     activation=activation_function,\n",
    "                     input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                     return_sequences=True))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "            model.add(LSTM(units, activation=activation_function, return_sequences=True))\n",
    "            model.add(LSTM(units, activation=activation_function, return_sequences=True))\n",
    "            model.add(LSTM(units, activation=activation_function))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "        elif layer[0] == 'SimpleRNN':\n",
    "            model.add(\n",
    "                SimpleRNN(units,\n",
    "                          activation=activation_function,\n",
    "                          input_shape=(X_train.shape[1], X_train.shape[2]),\n",
    "                          return_sequences=True))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "            model.add(SimpleRNN(units, activation=activation_function, return_sequences=True))\n",
    "            model.add(SimpleRNN(units, activation=activation_function, return_sequences=True))\n",
    "            model.add(SimpleRNN(units, activation=activation_function))\n",
    "            model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss=loss_function, optimizer=optimizer)\n",
    "    history = model.fit(X_train,\n",
    "                        y_train_df.to_numpy(),\n",
    "                        epochs=epoch,\n",
    "                        batch_size=batch_size,\n",
    "                        validation_data=(X_test, y_test_df.to_numpy()),\n",
    "                        verbose=1,\n",
    "                        shuffle=False)\n",
    "    return history, model\n",
    "\n",
    "\n",
    "def plot_history(history, title):\n",
    "    loss = history.history[\"loss\"]\n",
    "    val_loss = history.history[\"val_loss\"]\n",
    "    epochs = range(len(loss))\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, loss, \"b\", label=\"Train\")\n",
    "    plt.plot(epochs, val_loss, \"r\", label=\"Test\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\"images/history{}.jpg\".format(model_number))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_true_preds(true_value, predicted_value, train_test):\n",
    "    # train_test train için mi test için mi\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.scatter(true_value, predicted_value, c='crimson')\n",
    "    plt.yscale('log')\n",
    "    plt.xscale('log')\n",
    "    p1 = max(max(predicted_value), max(true_value))\n",
    "    p2 = min(min(predicted_value), min(true_value))\n",
    "    plt.plot([p1, p2], [p1, p2], 'b-')\n",
    "    plt.xlabel('True Values', fontsize=15)\n",
    "    plt.ylabel('Predictions', fontsize=15)\n",
    "    plt.axis('equal')\n",
    "    plt.savefig(\"images/true_preds{}_{}.jpg\".format(model_number, train_test))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluate_model(filled_data, target, model, X_train, X_test, y_test_df,\n",
    "                   y_train_df):\n",
    "    scaler_pred = MinMaxScaler()\n",
    "    scaler_pred.fit(filled_data[target].to_numpy().reshape(-1, 1))\n",
    "    # make predictions (test)\n",
    "    y_pred_test = model.predict(\n",
    "        X_test)  # yhat tahmin değerleri y_test ler ile kıyaslanacak.\n",
    "    # inverse scaling for forecast\n",
    "    # eğer y_pred test nan değeri içeriyorsa değiştirelim.\n",
    "    if np.any(np.isnan(y_pred_test)):\n",
    "        print(\"eksik değer var\")\n",
    "        y_pred_test = np.nan_to_num(y_pred_test)\n",
    "        np.savetxt(\"y_pred_test.csv\", y_pred_test, delimiter = \",\")\n",
    "    print(\"y_pred_test shape\", y_pred_test.shape)\n",
    "    inv_y_pred_test = scaler_pred.inverse_transform(y_pred_test)\n",
    "    y_true_test = scaler_pred.inverse_transform(y_test_df.to_numpy().reshape(\n",
    "        -1, 1))\n",
    "    # make predictions (train)\n",
    "    y_pred_train = model.predict(\n",
    "        X_train)  # yhat tahmin değerleri y_test ler ile kıyaslanacak.\n",
    "\n",
    "    if np.any(np.isnan(y_pred_train)):\n",
    "        print(\"eksik değer var\")\n",
    "        y_pred_train = np.nan_to_num(y_pred_train)\n",
    "        np.savetxt(\"y_pred_train.csv\", y_pred_train, delimiter = \",\")\n",
    "\n",
    "    # inverse scaling for forecast\n",
    "    inv_y_pred_train = scaler_pred.inverse_transform(y_pred_train)\n",
    "    y_true_train = scaler_pred.inverse_transform(y_train_df.to_numpy().reshape(\n",
    "        -1, 1))\n",
    "    \n",
    "    print(\"inv_y_pred_train dtype: \", inv_y_pred_train.dtype)\n",
    "    print(\"y_true_train dtype: \", y_true_train.dtype)\n",
    "    print(\"inv_y_pred_test dtype: \", inv_y_pred_test.dtype)\n",
    "    print(\"y_true_test dtype: \", y_true_test.dtype)\n",
    "\n",
    "    # TEST\n",
    "    test_mse = mean_squared_error(y_true_test, inv_y_pred_test)\n",
    "    test_mae = mean_absolute_error(y_true_test, inv_y_pred_test)\n",
    "    test_rmse = mean_squared_error(y_true_test, inv_y_pred_test) ** 0.5\n",
    "    test_r2 = r2_score(y_true_test, inv_y_pred_test)\n",
    "    test_mape = mean_absolute_percentage_error(y_true_test, inv_y_pred_test)\n",
    "    test_ME = max_error(y_true_test, inv_y_pred_test)\n",
    "    test_evs = explained_variance_score(y_true_test, inv_y_pred_test)\n",
    "\n",
    "    list_of_rows = []\n",
    "    test_perf = pd.Series({\n",
    "        'Set': 'test',\n",
    "        'MSE': test_mse,\n",
    "        'MAE': test_mae,\n",
    "        'RMSE': test_rmse,\n",
    "        'R2': test_r2,\n",
    "        'MAPE': test_mape,\n",
    "        'MAX_ERROR': test_ME,\n",
    "        'EXP_VAR_SCORE': test_evs\n",
    "    })\n",
    "    list_of_rows.append(test_perf)\n",
    "    # TRAIN\n",
    "\n",
    "\n",
    "\n",
    "    train_mse = mean_squared_error(y_true_train, inv_y_pred_train)\n",
    "    train_mae = mean_absolute_error(y_true_train, inv_y_pred_train)\n",
    "    train_rmse = mean_squared_error(y_true_train, inv_y_pred_train) ** 0.5\n",
    "    train_r2 = r2_score(y_true_train, inv_y_pred_train)\n",
    "    train_mape = mean_absolute_percentage_error(y_true_train, inv_y_pred_train)\n",
    "    train_ME = max_error(y_true_train, inv_y_pred_train)\n",
    "    train_evs = explained_variance_score(y_true_train, inv_y_pred_train)\n",
    "\n",
    "    train_perf = pd.Series({\n",
    "        'Set': 'train',\n",
    "        'MSE': train_mse,\n",
    "        'MAE': train_mae,\n",
    "        'RMSE': train_rmse,\n",
    "        'R2': train_r2,\n",
    "        'MAPE': train_mape,\n",
    "        'MAX_ERROR': train_ME,\n",
    "        'EXP_VAR_SCORE': train_evs\n",
    "    })\n",
    "    list_of_rows.append(train_perf)\n",
    "\n",
    "    df_results = pd.DataFrame(list_of_rows)\n",
    "    return df_results, inv_y_pred_test, y_true_test, inv_y_pred_train, y_true_train\n",
    "\n",
    "\n",
    "# Verilerin hazırlanması\n",
    "X_train, X_test, y_train_df, y_test_df = create_dataset(\"data/kagithane.xlsx\",\n",
    "                                                        cols=cols_to_analyze,\n",
    "                                                        n_hours=n_hours,\n",
    "                                                        target=target,\n",
    "                                                        train_size=train_size)\n",
    "# VERİ SETİ\n",
    "filled_data = pd.read_csv('data/filled_data.csv',\n",
    "                          index_col='date',\n",
    "                          parse_dates=True)\n",
    "\n",
    "# EĞİTİM\n",
    "print(\"Training.......\")\n",
    "start_time = datetime.now()\n",
    "# Öğrenme Aşaması\n",
    "history1, model1 = fit_model(layer_size, X_train, X_test, y_train_df,\n",
    "                             batch_size, epoch, units, activation_function,\n",
    "                             loss_function, optimizer, models)\n",
    "end_time = datetime.now()\n",
    "print('Duration: {}'.format(end_time - start_time))\n",
    "print(\"Eğitim Bitti. Model Kaydediliyor\")\n",
    "\n",
    "# EVALUATION\n",
    "df_results, inv_y_pred_test, y_true_test, inv_y_pred_train, y_true_train = evaluate_model(\n",
    "    filled_data, target, model1, X_train, X_test, y_test_df, y_train_df)\n",
    "\n",
    "first_date = pd.to_datetime('2015-01-02 00:00:00')\n",
    "row_size = filled_data.shape[0] - n_hours\n",
    "\n",
    "train_number = int(row_size * (train_size))\n",
    "test_number = row_size - train_number\n",
    "\n",
    "first_date = filled_data.index[n_hours]\n",
    "testin_basladigi_tarih = first_date + pd.to_timedelta(train_number, unit='h')\n",
    "\n",
    "test_indeksler = filled_data.loc[testin_basladigi_tarih:].index\n",
    "train_indeksler = filled_data.loc[first_date:(\n",
    "        testin_basladigi_tarih - pd.to_timedelta(1, unit='h'))].index\n",
    "\n",
    "test_true_pred_df = pd.DataFrame(\n",
    "    {\n",
    "        'y_true_test': y_true_test.flatten(),\n",
    "        'y_pred_test': inv_y_pred_test.flatten()\n",
    "    },\n",
    "    index=test_indeksler)\n",
    "train_true_pred_df = pd.DataFrame(\n",
    "    {\n",
    "        'y_true_train': y_true_train.flatten(),\n",
    "        'y_pred_train': inv_y_pred_train.flatten()\n",
    "    },\n",
    "    index=train_indeksler)\n",
    "\n",
    "# SAVINGS\n",
    "# save results to folder\n",
    "df_results.to_csv(r\"C:\\Users\\HP\\OneDrive\\Desktop\\PranCode\\hpo_pm_2_5-master\\df_results{}.csv\".format(model_number))\n",
    "test_true_pred_df.to_csv(\n",
    "    \"predictions/test_true_pred{}.csv\".format(model_number))\n",
    "train_true_pred_df.to_csv(\n",
    "    \"predictions/train_true_pred{}.csv\".format(model_number))\n",
    "\n",
    "# Save Model\n",
    "model_json = model1.to_json()\n",
    "with open(\"models/model{}.json\".format(model_number), \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model1.save_weights(\"models/model{}.h5\".format(model_number))\n",
    "print(\"Saved model to disk\")\n",
    "\n",
    "# save history\n",
    "print(\"history saved\")\n",
    "hist_df = pd.DataFrame(history1.history)\n",
    "hist_csv_file = 'histories/history{}.csv'.format(model_number)\n",
    "with open(hist_csv_file, mode='w') as f:\n",
    "    hist_df.to_csv(f)\n",
    "\n",
    "# PLOT RESULTS\n",
    "plot_true_preds(y_true_test, inv_y_pred_test, \"test\")\n",
    "plot_true_preds(y_true_train, inv_y_pred_train, \"train\")\n",
    "\n",
    "plot_history(history1, \"Training and Test Loss {}\".format(model_number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8aa374e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86405c61",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "celltoolbar": "Tags",
  "colab": {
   "collapsed_sections": [],
   "name": "kagithane_atm_met.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "a4ae173dc0bc553dffd3337b933bf6f05eed740245579b100d876076ba6fb9f0"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 505.912157,
   "end_time": "2022-01-28T23:01:31.746331",
   "environment_variables": {},
   "exception": true,
   "input_path": "input.ipynb",
   "output_path": "output_1.ipynb",
   "parameters": {
    "learning_rate": 0.1,
    "model_number": 1
   },
   "start_time": "2022-01-28T22:53:05.834174",
   "version": "2.3.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 567.841,
   "position": {
    "height": "40px",
    "left": "1010.09px",
    "right": "20px",
    "top": "101px",
    "width": "276px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
